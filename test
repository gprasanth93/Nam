Awesomeâ€”hereâ€™s the **updated, user-friendly internal blog** with the new **dual-tier bucketing (Compute C* + Storage S*)** and clear sizing guidance. It keeps the original rollout plan (tiny DBs auto-migrate to UAT next week; a week later to Prod) and adds practical examples, including the â€œsmall compute + huge diskâ€ case.

---

## ğŸš€ Upcoming Database Migration to KVM Firecracker â€“ What You Need to Know

**Dear PgMaker Users,**

Weâ€™re moving PgMaker databases to **Firecracker-based KVMs** to deliver **faster provisioning, stronger isolation, and more predictable performance**.
The rollout starts **next week** with tiny/low-IO databases in **UAT** and advances to **Production** the following week. Weâ€™ll work closely with your teams during each migration window and monitor everything end-to-end.

---

### ğŸ—“ï¸ Rollout at a Glance

| Phase        | When            | Environment    | What Happens                                                                    |
| ------------ | --------------- | -------------- | ------------------------------------------------------------------------------- |
| **Phase 1**  | Next week       | **UAT**        | Automatic migration of **tiny / low-IO** databases to Firecracker KVM.          |
| **Phase 2**  | +1 week         | **Prod**       | Automatic migration of **tiny / small** production databases.                   |
| **Phase 3+** | Following weeks | **UAT + Prod** | Gradual migration of **medium/large** databases in coordination with app teams. |

Weâ€™ll notify owners before each cutover, validate post-migration, and keep a rollback path ready.

---

## ğŸ§® How We Size: p95-Driven + Headroom

We size each KVM using **real usage data** (p95 CPU, RAM, IO, IOPS, and DB size) over the last 7â€“14 days, then add **25â€“50% headroom** for maintenance (VACUUM, checkpoints, basebackups) and bursts.
To make this clear and consistent, weâ€™re adopting a **dual-tier model**â€”just like the public clouds.

---

## ğŸ§± New Bucketing That Fits All: Compute + Storage (C*/S*)

We assign **one Compute Class (C*)** and **one Storage Class (S*)** per database.
This decouples CPU/RAM from disk capacity/performanceâ€”so edge cases like â€œ2 vCPU / 2 GB RAM / 3 TB diskâ€ are handled cleanly.

### ğŸ§  Compute Classes (C*)

Pick the smallest class that covers your **p95** CPU/RAM/connections; we provision with headroom.

| Class           | Target p95 vCPU | Target p95 RAM | Typical connections | Guidance                                        |
| --------------- | --------------: | -------------: | ------------------: | ----------------------------------------------- |
| **C0 â€“ Tiny**   |            â‰¤0.2 |          â‰¤2 GB |                 â‰¤50 | Dev/low traffic. Requires pgBouncer (txn mode). |
| **C1 â€“ Small**  |         0.2â€“0.6 |         2â€“6 GB |              50â€“150 | Light prod / APIs.                              |
| **C2 â€“ Medium** |         0.6â€“1.2 |        6â€“12 GB |             150â€“300 | Standard prod OLTP.                             |
| **C3 â€“ Large**  |         1.2â€“2.5 |       12â€“24 GB |             300â€“600 | Heavier OLTP / reporting.                       |
| **C4 â€“ XL**     |            >2.5 |         >24 GB |                >600 | Hot/mission-critical.                           |

**Provisioning rule of thumb**

* **vCPU** = `max(2, ceil(p95_cores Ã— 1.5))`
* **RAM** = `ceil(p95_ram_gib Ã— 1.30 + 0.75)`
  (If cache hit ratio <99% or swapping occurs, move up a class.)

---

### ğŸ’¾ Storage Classes (S*)

Choose independently based on **capacity needs** and **IO performance** (IOPS & MB/s).
We keep WAL on a **small, fast** SSD class.

| Class           | Capacity (DB+idx+WAL growth) | Perf (sustained) | Typical IOPS | Use case             |
| --------------- | ---------------------------: | ---------------: | -----------: | -------------------- |
| **S0 â€“ Tiny**   |                       â‰¤80 GB |          â‰¤6 MB/s |        ~1.5k | Dev/UAT              |
| **S1 â€“ Small**  |                      â‰¤160 GB |         â‰¤15 MB/s |          ~3k | Small prod           |
| **S2 â€“ Medium** |                      â‰¤300 GB |         â‰¤35 MB/s |          ~7k | Standard prod        |
| **S3 â€“ Large**  |                      â‰¤600 GB |         â‰¤75 MB/s |         ~15k | Heavy OLTP/reporting |
| **S4 â€“ XL**     |                        â‰¤1 TB |        â‰¤120 MB/s |         ~24k | High-volume prod     |
| **S5 â€“ XXL**    |             >1 TB (2â€“10 TB+) |     120â€“500 MB/s |    24kâ€“100k+ | Large/capacity-heavy |

**Provisioning rule of thumb**

* **Capacity** = live data + indexes + **40â€“60%** + WAL + temp + snapshots.
* **Throughput** = `ceil(p95_io_mb_s Ã— 1.5)`; **Burst** â‰ˆ `2Ã— sustained` for 30â€“60s.
* **IOPS (OLTP)** = `â‰¥ 2â€“4 Ã— p95_iops`.
* **WAL class**:

  * **W1 (standard fast SSD)** for most workloads
  * **W2 (very fast SSD)** for write-heavy / latency-sensitive systems

> This mirrors what major clouds provide (e.g., AWS **gp3/io2**, Azure **Premium SSD v2/Ultra**, GCP **Hyperdisk**)â€”performance can be tuned **independent of disk size**.

---

## ğŸ§© Examples (How C*/S* lands in real life)

* **Small compute, huge data (edge case)**
  *2 vCPU, 2â€“4 GB RAM, 3 TB disk, light IO* â†’ **C0 + S5**, WAL **W1/W2**

  * Compute stays small; storage gives big capacity with enough throughput (e.g., 250â€“500 MB/s) so maintenance jobs donâ€™t stall.
* **Standard OLTP**
  *4 vCPU, 8â€“12 GB RAM, 200â€“300 GB, ~20â€“30 MB/s p95* â†’ **C2 + S2**, WAL **W2**
* **Reporting-heavy scans**
  *6 vCPU, 24 GB RAM, 1 TB, high sequential reads* â†’ **C3 + S4**, prioritize throughput.

---

## ğŸ› ï¸ What Happens During Migration

1. PgMaker spins up a Firecracker KVM sized by your **C*+S*** class.
2. We seed it via **streaming replication** from your current instance.
3. After catch-up, we **promote** the KVM and **switch traffic** (brief interruption, usually <1 minute).
4. The old instance stays as **standby** until verification is done.

No endpoint or credential changes are required. Weâ€™ll watch metrics closely for 48â€“72 hours and adjust C*/S* if needed.

---

## ğŸ“Š After Migration: Visibility & Tuning

* Your DB shows up in the **PgMaker Metrics Dashboard** with the new KVM stats (CPU, RAM, disk throughput/IOPS, WAL, checkpoints).
* If we detect IO throttling >10%, low cache hit, or vacuum/checkpoint warnings, weâ€™ll **propose a class bump** (e.g., S2â†’S3 or C1â†’C2).

---

## âœ… What You Need to Do

* No manual action required for most teams.
* Avoid heavy schema changes during your scheduled window.
* Report any latency or error anomalies to **PgMaker Support** immediately.
* If your app has special timing/dependency constraints, ping us in advance.

---

## â“FAQ (Short)

**Q: Will performance improve immediately?**
**A:** Typically yesâ€”especially latency consistencyâ€”thanks to better isolation and tuned IO limits.

**Q: Can we resize without another migration?**
**A:** Yes. We can adjust compute and storage classes (C*/S*) in-place.

**Q: We have tiny compute but multi-TB dataâ€”are we forced to buy big VMs?**
**A:** No. Thatâ€™s the point of C*/S*: **small compute + large, right-sized storage**.

---

## ğŸ§­ Need Help?

* ğŸ“© **PgMaker Support**: *[team email / Slack channel]*
* ğŸ“Š **Rollout Tracker**: *[Confluence/Jira/URL]*
* ğŸ—“ï¸ **Info Session**: *[Date & Time]* (short walkthrough + Q&A)

---

### âœ¨ In Summary

* **Next week:** tiny/low-IO DBs auto-migrate to **UAT**;
* **Following week:** same group auto-migrates to **Prod**;
* We size with a **dual-tier model (C*/S*)** using your **p95 metrics + headroom**;
* No user changes needed; we partner closely during cutovers;
* Expect **better isolation, steadier latency, and right-sized cost**.

**â€“ The PgMaker Engineering Team**

---

Want this formatted for **Confluence** (callouts, collapsible â€œDetailsâ€ sections) or **email/HTML**? I can output a ready-to-paste version in your preferred format.
