# HostDetails 2.0: Snapshot-driven incident response + adaptive refresh

When a production host starts tipping into overload, the hardest part isn’t *knowing* it’s unhealthy — it’s answering the next question fast:

**“What exactly is causing this right now, and what can we safely stop to recover the box?”**

The new **HostDetails** experience is built for that exact moment. It combines a clean, at-a-glance resource view with **time-keyed TOP snapshots**, so responders can jump straight to the processes that were dominating CPU, memory, or I/O *at the incident timing* — not “whatever is running when you finally click refresh.”

---

## The pain point we’re solving

Traditional host dashboards often fail in the most critical window:

* You spot a spike, but by the time you inspect the host, the top process list has changed.
* You get one “current” view with no timeline, so you can’t correlate to the exact incident minute.
* Refresh is either too slow (misses the event) or too aggressive (wastes cycles on already-stressed hosts).

In real incidents, **minutes matter**, and **context matters even more**.

---

## What’s new in HostDetails

### 1) TOP snapshots that preserve “what was happening”

HostDetails now supports **multiple TOP snapshots** per host. Each snapshot is time-stamped and selectable, letting you:

* View top processes for **a specific moment**
* Compare “before vs during vs after” overload
* Avoid the “I refreshed and lost the smoking gun” problem

This is especially useful for:

* Sudden CPU storms (runaway query / batch / cron)
* Memory pressure causing swapping and cascading latency
* I/O saturation (log storms, vacuum bursts, runaway backups)

**Bottom line:** snapshots turn a transient overload into an **inspectable event**.

---

### 2) Incident-first workflow: identify → filter → act

HostDetails is optimized for the responder loop:

1. **Pick the incident timestamp snapshot**
2. Go straight to **Top Processes**
3. Use filters to isolate likely culprits:

   * **Filter by command** (quickly narrow by service/binary/job name)
   * **Filter/sort by CPU%**
   * **Filter/sort by MEM%**
4. Decide the safest mitigation:

   * Stop the runaway worker/job
   * Restart a bad actor service
   * Kill orphaned heavy processes
   * Escalate with concrete evidence (command + PID + resource usage)

This reduces guesswork and makes post-incident review far more objective.

---

## Adaptive refresh: fast when it matters, quiet when it doesn’t

Instead of refreshing on a fixed interval, HostDetails now supports **dynamic refresh based on current resource stress**. The refresh rate is driven by the **maximum** of CPU%, MEM%, or IO% (i.e., whichever is worst).

### Refresh policy (based on max(CPU, MEM, IO))

| Max usage band | Refresh interval     |
| -------------- | -------------------- |
| **< 50%**      | Every **4 hours**    |
| **50% – 60%**  | Every **2 hours**    |
| **60% – 70%**  | Every **1 hour**     |
| **70% – 80%**  | Every **30 minutes** |
| **> 80%**      | Every **5 minutes**  |

Why this matters:

* Healthy hosts aren’t constantly polled (less noise, less cost)
* Hot hosts get rapid updates automatically (faster detection + response)
* During sustained overload (>80%), the system “locks in” visibility by collecting **frequent snapshots**

---

## Snapshots as an incident timeline (example)

Imagine an outage window:

* **10:05** CPU jumps from 35% → 92%
* **10:07** latency alarms fire, traffic starts backing up
* **10:10** responder opens HostDetails

Without snapshots, you might only see the *current* top processes (which may have changed).
With snapshots, you can select:

* **10:05 snapshot** → identifies the first heavy command that triggered the spike
* **10:10 snapshot** → confirms whether the same command is still the dominant consumer
* **10:15 snapshot** → validates recovery after mitigation

This makes your actions defensible and repeatable: **“We stopped X because snapshot at 10:05 shows it consuming 78% CPU and 22 GB RSS.”**

---

## Better process exploration with filters

### Filter by command

This is the fastest way to answer:

* “Is this the app service, a batch job, a backup, a rogue script, or a DB task?”
* “Is one specific binary name repeating across the top list?”

### Filter by CPU and memory usage

These filters help you zoom in on:

* High CPU, low memory → tight loops, hot queries, busy polling
* High memory, moderate CPU → leaks, cache blowups, runaway JVM/node processes
* High I/O offenders (paired with IO metrics) → logging bursts, backups, vacuum churn

**Combined with snapshots**, these filters become a practical incident “search engine.”

---

## Why this is a big win for production operations

* **Faster MTTR:** responders spend less time hunting and more time fixing
* **Stronger evidence:** “snapshot at incident time” beats screenshots of a moving target
* **Lower overhead:** refresh rate scales with need (quiet when stable, fast when hot)
* **Better collaboration:** on-call can share a snapshot timestamp and everyone sees the same truth
* **Improved postmortems:** you can reconstruct what dominated the host during the incident window

---

## What’s next (easy extensions)

If we want to push this further, the natural next steps are:

* Auto-highlight “top offenders” (e.g., top 3 by CPU/MEM deltas vs baseline)
* Compare snapshots side-by-side (diff view)
* Add saved filters (“show only postgres”, “show only java”, “show only backup jobs”)
* Tie snapshot timestamps to alert timelines (click alert → open HostDetails at that snapshot)

---

## Closing

The upgraded HostDetails makes overload response **time-aware**, **actionable**, and **efficient**.
Snapshots ensure we don’t lose the truth of the incident moment, and adaptive refresh ensures we capture more detail automatically as stress rises — without wasting cycles when hosts are healthy.

If you want, paste your current HostDetails UI screenshot and I’ll tailor this blog with the exact component names/sections (and a “how to use it during an incident” walkthrough that matches your layout).
